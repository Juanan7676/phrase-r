When consulting a forecast, users often ask some variant of the following questions: Will an event of interest occur? If so, when will it occur? How long will it last? How intense will it be? Standard verification measures often do not directly communicate the ability of a forecast to answer these questions. Instead, quantitative scores typically address them indirectly or in some combined form. A more direct performance measure grew from what started as a project for a high-school intern. The challenge was to evaluate aspects of forecast quality from a set of convection-allowing (1.67 km) precipitation forecasts over Florida. Although the output was highly detailed, evaluation became manageable by simply adding a series of static landmarks with range rings and radials. Using the “targets” as a guide, the student and the two authors successfully obtained quantitative estimates of model tendencies that had heretofore only been reported anecdotally. What follows is a description of the method as well as the results from the analysis. It is hoped that this work will stimulate a broader discussion about how to extract performance information from very complex forecasts and present that information in terms that humans can readily perceive.
Coastal regions are increasingly vulnerable to damage from storm surge and inundation. Delft3D is used by the Naval Oceanographic Office to model the ocean dynamics in the near shore. In this study, the performance of Delft3D in predicting the surge and inundation during Hurricane Ike, which impacted the northern Gulf of Mexico in September 2008, is examined. Wave height, water level, and high-water mark comparisons with a number of observations confirm that the model does well in predicting the surge and inundation during extreme events. The impact of using forecast winds based on the best-track data as opposed to hindcast winds is also investigated, and it is found that the extent of inundation is represented reasonably well with the forecast winds. In Delft3D, waves can be coupled to the hydrodynamic component using the radiation stress gradient method or the dissipation method. Comparing the results of using the two shows that for low-resolution grids such as that needed for a forecast model the dissipation method works better at reproducing the water levels and inundation.
Recent studies document a polarimetric radar signature of refreezing. The signature is characterized by a low-level enhancement in differential reflectivity ZDR and a decrease in the copolar correlation coefficient ρhv within a region of decreasing radar reflectivity factor at horizontal polarization ZH toward the ground, called the refreezing layer (RFL). The evolution of the signature is examined during three winter storms in which the surface precipitation-type transitions from ice pellets to freezing rain. A modified quasi-vertical profile (QVP) technique is developed, which creates inverse-distance-weighted profiles using all available polarimetric data within a specified range from the radar location. Using this new technique reveals that the RFL descends in time prior to the transition from ice pellets to freezing rain and intersects the ground at the approximate transition time. Transition times are estimated using both crowdsourced and automated precipitation-type reports within a specified domain around the radar. These radar-estimated transition times are compared to a recently developed precipitation-classification algorithm based on Rapid Refresh (RAP) model wet-bulb temperature Tw profiles to explore potential benefits of analyzing QVPs during transition events. The descent of the RFL in the cases analyzed herein is related to low-level warm-air advection (WAA). A simple method for forecasting the transition time using QVPs is presented for cases of constant WAA. The repeatability of the refreezing signature’s descent in ice pellet to freezing rain transition events suggests the potential for its use in operational settings to create or modify short-term forecasts.
North Atlantic tropical cyclone (TC) forecasts from four ensemble prediction systems (EPSs) are verified using the National Hurricane Center’s (NHC) best tracks for the 2008–15 seasons. The 1–5-day forecasts are evaluated for the 21-member National Centers for Environmental Prediction (NCEP) Global Ensemble Forecast System (GEFS), the 23-member UKMO ensemble (UKMET), and the 51-member European Centre for Medium-Range Weather Forecasts (ECMWF) ensemble, as well as a combination of these ensembles [Multimodel Global (MMG)]. Several deterministic models are also evaluated, such as the Global Forecast System (GFSdet), Hurricane Weather Research and Forecasting Model (HWRF), the deterministic ECMWF model (ECdet), and the Geophysical Fluid Dynamical Laboratory model (GFDL).The ECdet track errors are the smallest on average at all lead times, but are not significantly different from the GEFS and ECMWF ensemble means. All models have a slow bias (90–240 km) in the along-track direction by 120 h, while there is little bias in the cross-track direction. Much of this slow bias is attributed to TCs undergoing extratropical transition (ET). All EPSs are underdispersed in the along-track direction, while the ECMWF is slightly overdispersed in the cross-track direction. The MMG and ECMWF track forecasts have more probabilistic skill than the ECdet and comparable skill to the NHC climatology-based cone forecast. TC intensity errors for the HWRF and GFDL are lower than the coarser models within the first 24 h, but are comparable to the ECdet at longer lead times. The ECMWF and MMG have comparable or better probabilistic intensity forecasts than the ECdet, while the GEFS’s weak bias limits its skill.
The development of an infrared (IR; specifically near 11 μm) eye probability forecast scheme for tropical cyclones is described. The scheme was developed from an eye detection algorithm that used a linear discriminant analysis technique to determine the probability of an eye existing in any given IR image given information about the storm center, motion, and latitude. Logistic regression is used for the model development and predictors were selected from routine information about the current storm (e.g., current intensity), forecast environmental factors (e.g., wind shear, oceanic heat content), and patterns/information (e.g., convective organization, tropical cyclone size) extracted from the current IR image. Forecasts were created for 6-, 12-, 18-, 24-, and 36-h forecast leads. Forecasts were developed using eye existence probabilities from North Atlantic tropical cyclone cases (1996–2014) and a combined North Atlantic and North Pacific (i.e., Northern Hemisphere) sample. The performance of North Atlantic–based forecasts, tested using independent eastern Pacific tropical cyclone cases (1996–2014), shows that the forecasts are skillful versus persistence at 12–36 h, and skillful versus climatology at 6–36 h. Examining the reliability and calibration of those forecasts shows that calibration and reliability of the forecasts is good for 6–18 h, but forecasts become a little overconfident at longer lead times. The forecasts also appear unbiased. The small differences between the Atlantic and Northern Hemisphere formulations are discussed. Finally, and remarkably, there are indications that smaller TCs are more prone to form eye features in all of the TC areas examined.
Statistical seasonal prediction of tropical cyclones (TCs) has been ongoing for quite some time in many different ocean basins across the world. While a few basins (e.g., North Atlantic and western North Pacific) have been extensively studied and forecasted for many years, Southern Hemispheric TCs have been less frequently studied and generally grouped as a whole or into two primary basins: southern Indian Ocean and Australian. This paper investigates the predictability of TCs in the northwest Australian (NWAUS) basin of the southeast Indian Ocean (105°–135°E) and describes two statistical approaches to the seasonal prediction of TC frequency, TC days, and accumulated cyclone energy (ACE). The first approach is a traditional deterministic seasonal prediction using predictors identified from NCEP–NCAR reanalysis fields using multiple linear regression. The second is a 100-member statistical ensemble approach with the same predictors as the deterministic model but with a resampling of the dataset with replacement and smearing input values to generate slightly different coefficients in the multiple linear regression prediction equations. Both the deterministic and ensemble schemes provide valuable forecasts that are better than climatological forecasts. The ensemble approach outperforms the deterministic model as well as adding quantitative uncertainty that reflects the predictability of a given TC season
The Global Ensemble Forecasting System (GEFS) is being extended from 16 to 35 days to cover the subseasonal period, bridging weather and seasonal forecasts. In this study, the impact of SST forcing on the extended-range land-only global 2-m temperature, continental United States (CONUS) accumulated precipitation, and MJO skill are explored with version 11 of the GEFS (GEFSv11) under various SST forcing configurations. The configurations consist of 1) the operational GEFS 90-day e-folding time of the observed real-time global SST (RTG-SST) anomaly relaxed to climatology, 2) an optimal AMIP configuration using the observed daily RTG-SST analysis, 3) a two-tier approach using the CFSv2-predicted daily SST, and 4) a two-tier approach using bias-corrected CFSv2-predicted SST, updated every 24 h. The experimental period covers the fall of 2013 and the winter of 2013/14. The results indicate that there are small differences in the ranked probability skill scores (RPSSs) between the various SST forcing experiments. The improvements in forecast skill of the Northern Hemisphere 2-m temperature and precipitation for weeks 3 and 4 are marginal, especially for North America. The bias-corrected CFSv2-predicted SST experiment generally delivers superior performance with statistically significant improvement in spatially and temporally aggregated 2-m temperature RPSSs over North America. Improved representation of the SST forcing (AMIP) increased the forecast skill for MJO indices up through week 2, but there is no significant improvement of the MJO forecast skill for weeks 3 and 4. These results are obtained over a short period with weak MJO activity and are also subject to internal model weaknesses in representing the MJO. Additional studies covering longer periods with upgraded model physics are warranted.
Thunderstorms in the United States cause over 100 deaths and $10 billion (U.S. dollars) in damage per year, much of which is attributable to straight-line (nontornadic) wind. This paper describes a machine-learning system that forecasts the probability of damaging straight-line wind (≥50 kt or 25.7 m s−1) for each storm cell in the continental United States, at distances up to 10 km outside the storm cell and lead times up to 90 min. Predictors are based on radar scans of the storm cell, storm motion, storm shape, and soundings of the near-storm environment. Verification data come from weather stations and quality-controlled storm reports. The system performs very well on independent testing data. The area under the receiver operating characteristic (ROC) curve ranges from 0.88 to 0.95, the critical success index (CSI) ranges from 0.27 to 0.91, and the Brier skill score (BSS) ranges from 0.19 to 0.65 (>0 is better than climatology). For all three scores, the best value occurs for the smallest distance (inside storm cell) and/or lead time (0–15 min), while the worst value occurs for the greatest distance (5–10 km outside storm cell) and/or lead time (60–90 min). The system was deployed during the 2017 Hazardous Weather Testbed.
Since 2007, meteorologists of the U.S. Army Test and Evaluation Command (ATEC) at Dugway Proving Ground (DPG), Utah, have relied on a mesoscale ensemble prediction system (EPS) known as the Ensemble Four-Dimensional Weather System (E-4DWX). This article describes E-4DWX and the innovative way in which it is calibrated, how it performs, why it was developed, and how meteorologists at DPG use it. E-4DWX has 30 operational members, each configured to produce forecasts of 48 h every 6 h on a 272-processor high performance computer (HPC) at DPG. The ensemble’s members differ from one another in initial-, lateral-, and lower-boundary conditions; in methods of data assimilation; and in physical parameterizations. The predictive core of all members is the Advanced Research core of the Weather Research and Forecasting (WRF) Model. Numerical predictions of the most useful near-surface variables are dynamically calibrated through algorithms that combine logistic regression and quantile regression, generating statistically realistic probabilistic depictions of the atmosphere’s future state at DPG’s observing sites. Army meteorologists view E-4DWX’s output via customized figures posted to a restricted website. Some of these figures summarize collective results—for example, through means, standard deviations, or fractions of the ensemble exceeding thresholds. Other figures show each forecast, individually or grouped—for example, through spaghetti diagrams and time series. This article presents examples of each type of figure.
In this paper, probabilistic wind speed forecasts are constructed based on ensemble numerical weather prediction (NWP) forecasts for both wind speed and wind direction. Including other NWP variables in addition to the one subject to forecasting is common for statistical calibration of deterministic forecasts. However, this practice is rarely seen for ensemble forecasts, probably because of a lack of methods. A Bayesian modeling approach (BMA) is adopted, and a flexible model class based on splines is introduced for the mean model. The spline model allows both wind speed and wind direction to be included nonlinearly. The proposed methodology is tested for forecasting hourly maximum 10-min wind speeds based on ensemble forecasts from the European Centre for Medium-Range Weather Forecasts at 204 locations in Norway for lead times from +12 to +108 h. An improvement in the continuous ranked probability score is seen for approximately 85% of the locations using the proposed method compared to standard BMA based on only wind speed forecasts. For moderate-to-strong wind the improvement is substantial, while for low wind speeds there is generally less or no improvement. On average, the improvement is 5%. The proposed methodology can be extended to include more NWP variables in the calibration and can also be applied to other variables.
What is the benefit of a near-convection-resolving ensemble over a near-convection-resolving deterministic forecast? In this paper, a way in which ensemble and deterministic numerical weather prediction (NWP) systems can be compared is demonstrated using a probabilistic verification framework. Three years’ worth of raw forecasts from the Met Office Unified Model (UM) 12-member 2.2-km Met Office Global and Regional Ensemble Prediction System (MOGREPS-UK) ensemble and 1.5-km Met Office U.K. variable resolution (UKV) deterministic configuration were compared, utilizing a range of forecast neighborhood sizes centered on surface synoptic observing site locations. Six surface variables were evaluated: temperature, 10-m wind speed, visibility, cloud-base height, total cloud amount, and hourly precipitation. Deterministic forecasts benefit more from the application of neighborhoods, though ensemble forecast skill can also be improved. This confirms that while neighborhoods can enhance skill by sampling more of the forecast, a single deterministic model state in time cannot provide the variability, especially at the kilometer scale, where rapid error growth acts to limit local predictability. Ensembles are able to account for the uncertainty at larger, synoptic scales. The results also show that the rate of decrease in skill with lead time is greater for the deterministic UKV. MOGREPS-UK retains higher skill for longer. The concept of a skill differential is introduced to find the smallest neighborhood size at which the deterministic and ensemble scores are comparable. This was found to be 3 × 3 (6.6 km) for MOGREPS-UK and 11 × 11 (16.5 km) for UKV. Comparable scores are between 2% and 40% higher for MOGREPS-UK, depending on the variable. Naively, this would also suggest that an extra 10 km in spatial accuracy is gained by using a kilometer-scale ensemble.
Many public safety officials (e.g., emergency managers and first responders) use weather-radar data to inform many life-saving decisions, such as sounding outdoor warning sirens and directing storm spotters. Therefore, to include this important user group in ongoing radar applications research, a knowledge coproduction framework is used to interact with, learn from, and provide information to public safety officials. From these interactions, it became clear that radar-based products that estimate a tornado’s location, intensity, or both could be valuable to public safety officials. Therefore, a survey was conducted and a focus group formed to 1) collect feedback on several of these products currently under development, 2) identify potential decisions that could be made with these products, and 3) examine the impact of radar update time on product usefulness. An analysis of the survey and focus group responses revealed that public safety officials preferred simple interactive products provided to them using multiple communication methods. Once received, any product that could clearly communicate where a tornado may have occurred would likely help public safety officials focus search and rescue efforts in the immediate aftermath of a tornado. Additionally, public safety officials preferred products created using rapid-update data (1–2-min volumetric updates) over conventional-update data (4–5-min volumetric updates) because it provided them with more complete information.
Lake-effect snow (LES) is a cold-season mesoscale convective phenomenon that can lead to significant snowfall rates and accumulations in the Great Lakes region of the United States. While limited-area numerical weather prediction models have shown skill in prediction of warm-season convective storms, forecasting the sharp nature of LES precipitation timing, intensity, and location is difficult because of model error and initial and boundary condition uncertainties. Ensemble forecasting can incorporate and quantify some sources of forecast error, but ensemble design must be considered. This study examines the relative contributions of forecast uncertainties to LES forecast error using a regional convection-allowing data assimilation and ensemble prediction system. Ensembles are developed using various methods of perturbations to simulate a long-lived and high-precipitation LES event in December 2013, and forecast performance is evaluated using observations including those from the Ontario Winter Lake-Effect Systems (OWLeS) campaign. Model lateral boundary conditions corresponding to weather conditions beyond the Great Lakes region play an influential role in LES precipitation forecasts and their uncertainty, as evidenced by ensemble spread, particularly at lead times beyond one day. A strong forecast dependence on regional initial conditions was shown using data assimilation. This sensitivity impacts the timing and intensity of predicted precipitation, as well as band location and orientation assessed with an object-based verification approach, giving insight into the time scales of practical predictability of LES. Overall, an assimilation-cycling convection-allowing ensemble prediction system could improve future lake-effect snow precipitation forecasts and analyses and can help quantify and understand sources of forecast uncertainty.
Through a case study of Hurricane Arthur (2014), the Weather Research and Forecasting (WRF) Model and the Finite Volume Coastal Ocean Model (FVCOM) are used to investigate the sensitivity of storm surge forecasts to physics parameterizations and configurations of the initial and boundary conditions in WRF. The turbulence closure scheme in the planetary boundary layer affects the prediction of the storm intensity: the local closure scheme produces lower equivalent potential temperature than the nonlocal closure schemes, leading to significant reductions in the maximum surface wind speed and surge heights. On the other hand, higher-class cloud microphysics schemes overpredict the wind speed, resulting in large overpredictions of storm surge at some coastal locations. Without cumulus parameterization in the outermost domain, both the wind speed and storm surge are grossly underpredicted as a result of large precipitation decreases in the storm center. None of the choices for the WRF physics parameterization schemes significantly affect the prediction of Arthur’s track. Sea surface temperature affects the latent heat release from the ocean surface and thus storm intensity and storm surge predictions. The large-scale atmospheric circulation models provide the initial and boundary conditions for WRF, and influence both the track and intensity predictions, thereby changing the spatial distribution of storm surge along the coastline. These sensitivity analyses underline the need to use an ensemble modeling approach to improve the storm surge forecasts.
This study is an aviation-based application of NOAA’s second-generation medium-range Global Ensemble Forecast System Reforecast (GEFS/R; i.e., hindcast or retrospective forecast) dataset. The study produced a downscaled probabilistic prediction of instrument flight conditions at major U.S. airports using an analog approach. This represents an initial step toward applications of reforecast data to probabilistic aviation decision support services. Results from this study show that even at the very coarse resolution of the GEFS/R dataset, the analog approach yielded skillful probabilistic forecasts of flight conditions (i.e., instrument flight rules vs visual flight rules) at most of the Federal Aviation Administration (FAA)’s Core 30 airports. This was particularly true over the central and eastern United States, including the important Golden Triangle, where aircraft flow affects traffic flow management across the entire national airspace system. Additionally, the results suggest that reforecast systems utilizing better horizontal and vertical resolution, in both the modeling system and the reforecast archive, would be very useful for aviation forecasting applications
This study describes the initial application of radiance bias correction and channel selection in the hourly updated Rapid Refresh model. For this initial application, data from the Atmospheric Infrared Sounder (AIRS) are used; this dataset gives atmospheric temperature and water vapor information at higher vertical resolution and accuracy than previously launched low-spectral resolution satellite systems. In this preliminary study, data from AIRS are shown to add skill to short-range weather forecasts over a relatively data-rich area. Two 1-month retrospective runs were conducted to evaluate the impact of assimilating clear-sky AIRS radiance data on 1–12-h forecasts using a research version of the National Oceanic and Atmospheric Administration (NOAA) Rapid Refresh (RAP) regional mesoscale model already assimilating conventional and other radiance [AMSU-A, Microwave Humidity Sounder (MHS), HIRS-4] data. Prior to performing the assimilation, a channel selection and bias-correction spinup procedure was conducted that was appropriate for the RAP configuration. RAP forecasts initialized from analyses with and without AIRS data were verified against radiosonde, surface atmosphere, precipitation, and satellite radiance observations. Results show that the impact from AIRS radiance data on short-range forecast skill in the RAP system is small but positive and statistically significant at the 95% confidence level. The RAP-specific channel selection and bias correction procedures described in this study were the basis for similar applications to other radiance datasets now assimilated in version 3 of RAP implemented at NOAA’s National Centers for Environmental Prediction (NCEP) in August 2016.
Focusing on afternoon thunderstorms in Taiwan during the warm season (May–October) under weak synoptic forcing, this study applied the Taiwan Auto-NowCaster (TANC) to produce 1-h likelihood nowcasts of afternoon convection initiation (ACI) using a fuzzy logic approach. The primary objective is to design more useful forecast products with uncertainty regions of predicted thunderstorms to provide nowcast guidance of ACI for forecasters. Four sensitivity tests on forecast performance were conducted to improve the usefulness of nowcasts for forecasters. The optimal likelihood threshold (Lt) for ACIs, which is the likelihood value that best corresponds to the observed ACIs, was determined to be 0.6. Because of the high uncertainty on the exact location or timing of ACIs in nowcasts, location displacement and temporal shifting of ACIs should be considered in operational applications. When a spatial window of 5 km and a temporal window of 18 min are applied, the TANC displays moderate accuracy and satisfactory discrimination with an acceptable degree of overforecasting. The nonparametric Mann–Whitney test indicated that the performance of the TANC substantially surpasses the competing Space and Time Multiscale Analysis System–Weather Research and Forecasting Model, which serves as a pertinent reference for short-range (0–6 h) forecasts at the Central Weather Bureau in Taiwan.
Forecasting severe hail accurately requires predicting how well atmospheric conditions support the development of thunderstorms, the growth of large hail, and the minimal loss of hail mass to melting before reaching the surface. Existing hail forecasting techniques incorporate information about these processes from proximity soundings and numerical weather prediction models, but they make many simplifying assumptions, are sensitive to differences in numerical model configuration, and are often not calibrated to observations. In this paper a storm-based probabilistic machine learning hail forecasting method is developed to overcome the deficiencies of existing methods. An object identification and tracking algorithm locates potential hailstorms in convection-allowing model output and gridded radar data. Forecast storms are matched with observed storms to determine hail occurrence and the parameters of the radar-estimated hail size distribution. The database of forecast storms contains information about storm properties and the conditions of the prestorm environment. Machine learning models are used to synthesize that information to predict the probability of a storm producing hail and the radar-estimated hail size distribution parameters for each forecast storm. Forecasts from the machine learning models are produced using two convection-allowing ensemble systems and the results are compared to other hail forecasting methods. The machine learning forecasts have a higher critical success index (CSI) at most probability thresholds and greater reliability for predicting both severe and significant hail
The High Resolution Rapid Refresh (HRRR) model has been the National Weather Service’s (NWS) operational rapid update model since 2014. The HRRR has undergone continual development, including updates to the Weather Research and Forecasting (WRF) Model core, the data assimilation system, and the various physics packages in order to better represent atmospheric processes, with updated operational versions of the model being implemented approximately every spring. Given the model’s intent for use in convective precipitation forecasting, it is of interest to examine how forecasts of warm season precipitation have changed as a result of the continued model upgrades. A features-based assessment is performed on the first 6 h of HRRR quantitative precipitation forecasts (QPFs) from the 2013, 2014, and 2015 versions of the model over the U.S. central plains in an effort to understand how specific aspects of QPF performance have evolved as a result of continued model development. Significant bias changes were found with respect to precipitation intensity. Model upgrades that increased boundary layer stability and reduced the strength of the latent heating perturbations in the data assimilation were found to reduce southward biases in convective initiation, reduce the tendency for the model to overestimate heavy rainfall, and improve the representation of convective initiation.
Southeast U.S. cold season severe weather events can be difficult to predict because of the marginality of the supporting thermodynamic instability in this regime. The sensitivity of this environment to prognoses of instability encourages additional research on ways in which mesoscale models represent turbulent processes within the lower atmosphere that directly influence thermodynamic profiles and forecasts of instability. This work summarizes characteristics of the southeast U.S. cold season severe weather environment and planetary boundary layer (PBL) parameterization schemes used in mesoscale modeling and proceeds with a focused investigation of the performance of nine different representations of the PBL in this environment by comparing simulated thermodynamic and kinematic profiles to observationally influenced ones. It is demonstrated that simultaneous representation of both nonlocal and local mixing in the Asymmetric Convective Model, version 2 (ACM2), scheme has the lowest overall errors for the southeast U.S. cold season tornado regime. For storm-relative helicity, strictly nonlocal schemes provide the largest overall differences from observationally influenced datasets (underforecast). Meanwhile, strictly local schemes yield the most extreme differences from these observationally influenced datasets (underforecast) in a mean sense for the low-level lapse rate and depth of the PBL, on average. A hybrid local–nonlocal scheme is found to mitigate these mean difference extremes. These findings are traced to a tendency for local schemes to incompletely mix the PBL while nonlocal schemes overmix the PBL, whereas the hybrid schemes represent more intermediate mixing in a regime where vertical shear enhances mixing and limited instability suppresses mixing.
Eight years of daily, experimental, deterministic, convection-allowing model (CAM) forecasts, produced by the National Severe Storms Laboratory, were evaluated to assess their ability at predicting severe weather hazards over a diverse collection of seasons, regions, and environments. To do so, forecasts of severe weather hazards were produced and verified as in previous studies using CAM output, namely by thresholding the updraft helicity (UH) field, smoothing the resulting binary field to create surrogate severe probability forecasts (SSPFs), and verifying the SSPFs against observed storm reports. SSPFs were most skillful during the spring and fall, with a relative minimum in skill observed during the summer. SSPF skill during the winter months was more variable than during other seasons, partly due to the limited sample size of events, but was often less than that during the warm season. The seasonal behavior of SSPF skill was partly driven by the relationship between the UH threshold and the likelihood of obtaining severe storm reports. Varying UH thresholds by season and region produced SSPFs that were more skillful than using a fixed UH threshold to identify severe convection. Accounting for this variability was most important during the cool season, when a lower UH threshold produced larger SSPF skill compared to warm-season events, and during the summer, when large differences in skill occurred within different parts of the continental United States (CONUS), depending on the choice of UH threshold. This relationship between UH threshold and SSPF skill is discussed within the larger scope of generating skillful CAM-based guidance for hazardous convective weather and verifying CAM predictions.
This study identifies high-impact severe weather events with poor predictive skill over the northeast United States using Storm Prediction Center (SPC) convective outlooks. The objectives are to build a climatology of high-impact, low predictive skill events between 1980 and 2013 and investigate the differences in the synoptic-scale environment and severe weather parameters between severe weather events with low predictive skill and high predictive skill. Event-centered composite analyses, performed using the National Centers for Environmental Prediction Climate Forecast System Reanalysis and the North American Regional Reanalysis, suggest low predictive skill events occur significantly more often in low-shear environments. Additionally, a plurality of low probability of detection (POD), high-impact events occurred in low-shear, high-CAPE environments. Statistical analysis of low-shear, high-CAPE environments suggests high downdraft CAPE (DCAPE) and relatively dry lower levels of the atmosphere are associated with widespread severe weather events. DCAPE and dry boundary layer air may contribute to severe wind gusts through strong negative buoyancy and enhanced evaporative cooling of descending saturated parcels.
Accurate prediction of storm surge is a difficult problem. Most forecast systems produce multiple possible forecasts depending on the variability in weather conditions, possible temperature levels, winds, etc. Ensemble modeling techniques have been developed with the stated purpose of obtaining the best forecast (in some specific sense) from the individual forecasts. In this work a statistical methodology of evaluating the performance of multiple ensemble forecasting models is developed. The methodology is applied to predicting storm surge in the New York Harbor area. Data from three hurricane events collected from multiple locations in the New York Bay area are used. The methodology produces three key findings for the particular test data used. First, it is found that even the simplest possible way of creating an ensemble produces results superior to those of any single forecast. Second, for the data used and the events under study the methodology did not interact with any event at any location studied. Third, based on the methodology results for the data studied selecting the best-performing ensemble models for each specific location may be possible.
Among forecasters and storm chasers, there is a common perception that hodographs with counterclockwise curvature or kinking in the midlevels (sometimes called backing aloft or veer–back–veer profiles) are unfavorable for long-lived supercells and tornadoes. This study reviews and then evaluates several possible explanations for the purported negative effect of backing aloft. As a controlled hypothesis test, simulated supercells are initiated within a range of idealized wind profiles, many of which include representative counterclockwise kinks or bends in their hodographs. In these experiments, the short-term, direct impacts of backing aloft upon supercell maintenance are generally small. Backing aloft does modify the component of vertical accelerations linked to updraft–shear interactions, but these changes generally occur well above the level of free convection (LFC), and they are generally offset by substantial upward accelerations attributable to other processes (e.g., within-storm rotation and positive buoyancy). In these simulations, the longevity of isolated supercells seems to be most directly hindered in environments with very low storm-relative helicity (SRH) or else (for a line of supercells) substantial along-line flow in the upper troposphere. Although these two disrupting properties can accompany backing aloft, they are neither universally nor exclusively associated with it. From the perspective of storm dynamics, it seems advisable to focus on SRH and along-line flow in the environment, rather than the presence (or absence) of backing aloft in the wind profile.
Thirty National Weather Service forecasters worked with 1-, 2-, and 5-min phased-array radar (PAR) volumetric updates for a variety of weather events during the 2015 Phased Array Radar Innovative Sensing Experiment. Exposure to each of these temporal resolutions during simulated warning operations meant that these forecasters could provide valuable feedback on how rapidly updating PAR data impacted their warning decision processes. To capture this feedback, forecasters participated in one of six focus groups. A series of open-ended questions guided focus group discussions, and forecasters were encouraged to share their experiences and opinions from the experiment. Transcriptions of focus group discussions were thematically analyzed, and themes belonging to one of two groups were identified: 1) forecasters’ use of rapidly updating PAR data during the experiment and 2) how forecasters envision rapidly updating PAR data being integrated into warning operations. Findings from this thematic analysis are presented in this paper, and to illustrate these findings from the forecasters’ perspectives, dialogue that captures the essence of their discussions is shared. The identified themes provide motivation to integrate rapidly updating radar data into warning operations and highlight important factors that need to be addressed for the successful integration of these data.
In this study, an optimal weighting system is developed that combines multiple seasonal probabilistic forecasts in the North American Multimodel Ensemble (NMME). The system is applied to predict temperature and precipitation over the North American continent, and the analysis is conducted using the 1982–2010 hindcasts from eight NMME models, including the CFSv2, CanCM3, CanCM4, GFDL CM2.1, Forecast-Oriented Low Ocean Resolution (FLOR), GEOS5, CCSM4, and CESM models, with weights determined by minimizing the Brier score using ridge regression. Strategies to improve the performance of ridge regression are explored, such as eliminating a priori models with negative skill and increasing the effective sample size by pooling information from neighboring grids. A set of constraints is put in place to confine the weights within a reasonable range or restrict the weights from departing wildly from equal weights. So when the predictor–predictand relationship is weak, the multimodel ensemble forecast returns to an equal-weight combination. The new weighting system improves the predictive skill from the baseline, equally weighted forecasts. All models contribute to the weighted forecasts differently based upon location and forecast start and lead times. The amount of improvement varies across space and corresponds to the average model elimination percentage. The areas with higher elimination rates tend to show larger improvement in cross-validated verification scores. Some local improvements can be as large as 0.6 in temporal probability anomaly correlation (TPAC). On average, the results are about 0.02–0.05 in TPAC for temperature probabilistic forecasts and 0.03–0.05 for precipitation probabilistic forecasts over North America. The skill improvement is generally greater for precipitation probabilistic forecasts than for temperature probabilistic forecasts.