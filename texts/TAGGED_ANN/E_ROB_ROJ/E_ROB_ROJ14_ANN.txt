<aim3>This paper presents a visual-inertial odometry framework that tightly fuses inertial measurements with visual data from one or more cameras, by means of an iterated extended Kalman filter </aim3>. <meth>By employing image patches as landmark descriptors, a photometric error is derived, which is directly integrated as an innovation term in the filter update step </meth>. <concl>Consequently, the data association is an inherent part of the estimation process and no additional feature extraction or matching processes are required </concl>. <meth2>Furthermore, it enables the tracking of noncorner-shaped features, such as lines, and thereby increases the set of possible landmarks </meth2>. <meth2>The filter state is formulated in a fully robocentric fashion, which reduces errors related to nonlinearities </meth2>. <meth2>This also includes partitioning of a landmark’s location estimate into a bearing vector and distance and thereby allows an undelayed initialization of landmarks</meth2>. <concl>Overall, this results in a compact approach, which exhibits a high level of robustness with respect to low scene texture and motion blur </concl>. <concl>Furthermore, there is no time-consuming initialization procedure and pose estimates are available starting at the second image frame </concl>. <analy>We test the filter on different real datasets and compare it with other state-of-the-art visual-inertial frameworks </analy>. <concl>Experimental results show that robust localization with high accuracy can be achieved with this filter-based framework </concl>.