When consulting a forecast, users often ask some variant of the following questions: Will an event of interest occur? If so, when will it occur? How long will it last? How intense will it be? Standard verification measures often do not directly communicate the ability of a forecast to answer these questions. Instead, quantitative scores typically address them indirectly or in some combined form. A more direct performance measure grew from what started as a project for a high-school intern. The challenge was to evaluate aspects of forecast quality from a set of convection-allowing (1.67 km) precipitation forecasts over Florida. Although the output was highly detailed, evaluation became manageable by simply adding a series of static landmarks with range rings and radials. Using the “targets” as a guide, the student and the two authors successfully obtained quantitative estimates of model tendencies that had heretofore only been reported anecdotally. What follows is a description of the method as well as the results from the analysis. It is hoped that this work will stimulate a broader discussion about how to extract performance information from very complex forecasts and present that information in terms that humans can readily perceive.